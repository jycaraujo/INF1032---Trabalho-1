{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.4              0.70         0.00             1.9      0.076   \n",
       "1            7.8              0.88         0.00             2.6      0.098   \n",
       "2            7.8              0.76         0.04             2.3      0.092   \n",
       "3           11.2              0.28         0.56             1.9      0.075   \n",
       "4            7.4              0.70         0.00             1.9      0.076   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
       "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
       "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
       "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      9.4        5  \n",
       "1      9.8        5  \n",
       "2      9.8        5  \n",
       "3      9.8        6  \n",
       "4      9.4        5  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_file = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "wine_raw = pd.read_csv(csv_file, sep=\";\")\n",
    "wine_data = wine_raw\n",
    "wine_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 4, 5, 6, 7, 8]\n",
      "1279 1279\n",
      "320 320\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn import neighbors\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "import math\n",
    "\n",
    "# Definir seed utilizada\n",
    "random.seed(1001001)\n",
    "\n",
    "# Definir as features\n",
    "all_features = ['fixed acidity', 'volatile acidity', 'citric acid', \n",
    "            'residual sugar', 'chlorides', 'free sulfur dioxide', \n",
    "            'total sulfur dioxide', 'density', 'pH', \n",
    "            'sulphates', 'alcohol', 'quality']\n",
    "features = ['fixed acidity', 'volatile acidity', 'citric acid', \n",
    "            'residual sugar', 'chlorides', 'free sulfur dioxide', \n",
    "            'total sulfur dioxide', 'density', 'pH', \n",
    "            'sulphates', 'alcohol']\n",
    "outcome_column = 'quality'\n",
    "\n",
    "outcome_labels = sorted(list(set(wine_data.quality)))\n",
    "print(outcome_labels)\n",
    "\n",
    "X = np.array(wine_data[features])\n",
    "Y = np.array(wine_data[outcome_column])\n",
    "\n",
    "# Separar os conjuntos de treino e teste\n",
    "Wine_Train, Wine_Test = train_test_split(wine_data, test_size=0.2, stratify=wine_data[outcome_column])\n",
    "\n",
    "# Converter os conjuntos de treino para array\n",
    "X_train = np.array(Wine_Train[features])\n",
    "Y_train = np.array(Wine_Train[outcome_column])\n",
    "print(X_train.shape[0], Y_train.shape[0])\n",
    "\n",
    "# Converter os conjuntos de teste para array\n",
    "X_test = np.array(Wine_Test[features])\n",
    "Y_test = np.array(Wine_Test[outcome_column])\n",
    "print(X_test.shape[0], Y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "Predicted value: 5 , real target: 6\n",
      "\n",
      "\n",
      "# Precision\n",
      "#\n",
      "# Training Data\n",
      "Average of all labelsPrecision: 0.5863\n",
      "OverallPrecision: 0.6677\n",
      "Precision By Label\n",
      "Label: 3\t1.0000\n",
      "Label: 4\t0.5000\n",
      "Label: 5\t0.6672\n",
      "Label: 6\t0.6701\n",
      "Label: 7\t0.6807\n",
      "Label: 8\t0.0000\n",
      "#\n",
      "# Test Data\n",
      "Average of all labelsPrecision: 0.2313\n",
      "OverallPrecision: 0.5094\n",
      "Precision By Label\n",
      "Label: 3\t0.0000\n",
      "Label: 4\t0.0000\n",
      "Label: 5\t0.5593\n",
      "Label: 6\t0.4754\n",
      "Label: 7\t0.3529\n",
      "Label: 8\t0.0000\n",
      "\n",
      "\n",
      "# Recall\n",
      "#\n",
      "# Training Data\n",
      "Average of all labelsRecall: 0.4118\n",
      "OverallRecall: 0.6677\n",
      "Recall By Label\n",
      "Label: 3\t0.3750\n",
      "Label: 4\t0.1429\n",
      "Label: 5\t0.7982\n",
      "Label: 6\t0.6451\n",
      "Label: 7\t0.5094\n",
      "Label: 8\t0.0000\n",
      "#\n",
      "# Test Data\n",
      "Average of all labelsRecall: 0.2218\n",
      "OverallRecall: 0.5094\n",
      "Recall By Label\n",
      "Label: 3\t0.0000\n",
      "Label: 4\t0.0000\n",
      "Label: 5\t0.7279\n",
      "Label: 6\t0.4531\n",
      "Label: 7\t0.1500\n",
      "Label: 8\t0.0000\n",
      "\n",
      "\n",
      "# F1 Score\n",
      "#\n",
      "# Training Data\n",
      "Average of all labelsF1 Score: 0.4558\n",
      "OverallF1 Score: 0.6677\n",
      "F1 Score By Label\n",
      "Label: 3\t0.5455\n",
      "Label: 4\t0.2222\n",
      "Label: 5\t0.7268\n",
      "Label: 6\t0.6573\n",
      "Label: 7\t0.5827\n",
      "Label: 8\t0.0000\n",
      "#\n",
      "# Test Data\n",
      "Average of all labelsF1 Score: 0.2179\n",
      "OverallF1 Score: 0.5094\n",
      "F1 Score By Label\n",
      "Label: 3\t0.0000\n",
      "Label: 4\t0.0000\n",
      "Label: 5\t0.6326\n",
      "Label: 6\t0.4640\n",
      "Label: 7\t0.2105\n",
      "Label: 8\t0.0000\n",
      "\n",
      "\n",
      "# Accuracy\n",
      "#\n",
      "# Training Data\n",
      "Accuracy: 0.6677\n",
      "#\n",
      "# Test Data\n",
      "Accuracy: 0.5094\n",
      "accuracy in training data: 0.6677\n",
      "accuracy in test data:     0.5094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabi_\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\gabi_\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "#Create an instance of K-nearest neighbor classifier\n",
    "knn_model = neighbors.KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "#Train the classifier\n",
    "knn_model.fit(X_train,Y_train)\n",
    "\n",
    "Yhat_train = knn_model.predict(X_train)\n",
    "\n",
    "#Compute the prediction according to the model\n",
    "Yhat = knn_model.predict(X_test)\n",
    "print(Yhat[4])\n",
    "print ('Predicted value: ' + str(Yhat[-1]), ', real target: ' + str(Y[-1]))\n",
    "\n",
    "_Precision = 1\n",
    "_Recall = 2\n",
    "_F1Score = 3\n",
    "_Accuracy = 4\n",
    "\n",
    "_AverageOfAllScores = 1 # Calcula a precisão/f1_score/recall de cada label e depois faz a média delas\n",
    "_OverallScore = 2 # Calcula a precisão/f1_score/recall global\n",
    "_ScoreByLabel = 3 # Calcula a precisão/f1_score/recall de cada label e retorna elas em um array\n",
    "\n",
    "def calculateScores(real_group, pred_group, score_type, average_type):\n",
    "    if(average_type == _AverageOfAllScores):\n",
    "        chosenAverage = \"macro\"\n",
    "    elif(average_type == _OverallScore):\n",
    "        chosenAverage = \"micro\"\n",
    "    elif(average_type == _ScoreByLabel):\n",
    "        chosenAverage = None\n",
    "    \n",
    "    if(score_type == _Precision):\n",
    "        return metrics.precision_score(y_true=real_group, y_pred=pred_group, average=chosenAverage)\n",
    "    elif(score_type == _Recall):\n",
    "        return metrics.recall_score(y_true=real_group, y_pred=pred_group, average=chosenAverage)\n",
    "    elif(score_type == _F1Score):\n",
    "        return metrics.f1_score(y_true=real_group, y_pred=pred_group, average=chosenAverage)\n",
    "    elif(score_type == _Accuracy):\n",
    "        # Normalizar os dados: Calcula a acurácia\n",
    "        # Sem normalizar os dados: Calcula o número total de TP\n",
    "        return metrics.accuracy_score(y_true=real_group, y_pred=pred_group, normalize=True)\n",
    "    \n",
    "    \n",
    "scores = ['Precision', 'Recall', 'F1 Score', 'Accuracy']\n",
    "\n",
    "def showScoreResults(score_name, real_group, pred_group):\n",
    "    if(score_name == scores[0]):\n",
    "        method = _Precision\n",
    "    elif(score_name == scores[1]):\n",
    "        method = _Recall\n",
    "    elif(score_name == scores[2]):\n",
    "        method = _F1Score\n",
    "    elif(score_name == scores[3]):\n",
    "        method = _Accuracy\n",
    "        \n",
    "    if(method == _Precision or method == _Recall or method == _F1Score):\n",
    "        score = calculateScores(real_group, pred_group, method, _AverageOfAllScores)\n",
    "        print('Average of all labels' + score_name + ':', '{:6.4f}'.format(score))\n",
    "        score = calculateScores(real_group, pred_group, method, _OverallScore)\n",
    "        print('Overall' + score_name + ':', '{:6.4f}'.format(score))\n",
    "        score = calculateScores(real_group, pred_group, method, _ScoreByLabel)\n",
    "        print(score_name + ' By Label')\n",
    "        for label in outcome_labels:\n",
    "            index = outcome_labels.index(label)\n",
    "            print('Label: ' + str(label) + '\\t' + '{:6.4f}'.format(score[index]))\n",
    "    elif(method == _Accuracy):\n",
    "        score = calculateScores(real_group, pred_group, method, 0)\n",
    "        print(score_name + ':', '{:6.4f}'.format(score))\n",
    "\n",
    "def showScores_TrainAndTest():\n",
    "    for score in scores:\n",
    "        print('\\n')\n",
    "        print('# ' + score)\n",
    "        print('#')\n",
    "        print('# Training Data')\n",
    "        showScoreResults(score, Y_train, Yhat_train)\n",
    "        print('#')\n",
    "        print('# Test Data')\n",
    "        showScoreResults(score, Y_test, Yhat)\n",
    "        \n",
    "showScores_TrainAndTest(); \n",
    "\n",
    "\n",
    "# f1_score = metrics.f1_score(y_true=Y_train, y_pred=Yhat_train, average=\"micro\")\n",
    "# print('f1_score in training data:', '{:6.4f}'.format(f1_score))\n",
    "# f1_score = metrics.f1_score(y_true=Y_test, y_pred=Yhat, average=\"micro\")\n",
    "# print('f1_score in test data:', '{:6.4f}'.format(f1_score))\n",
    "\n",
    "# ac = metrics.accuracy_score(y_true=Y_train, y_pred=Yhat_train)\n",
    "# print('ac in training data:', '{:6.4f}'.format(ac))\n",
    "# ac = metrics.accuracy_score(y_true=Y_test, y_pred=Yhat)\n",
    "# print('ac in test data:', '{:6.4f}'.format(ac))\n",
    "\n",
    "accuracy_train = knn_model.score(X_train, Y_train)\n",
    "print('accuracy in training data:', '{:6.4f}'.format(accuracy_train))\n",
    "accuracy_test = knn_model.score(X_test, Y_test)\n",
    "print('accuracy in test data:    ', '{:6.4f}'.format(accuracy_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>true_3</th>\n",
       "      <th>true_4</th>\n",
       "      <th>true_5</th>\n",
       "      <th>true_6</th>\n",
       "      <th>true_7</th>\n",
       "      <th>true_8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pred_3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pred_4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pred_5</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>99</td>\n",
       "      <td>62</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pred_6</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>34</td>\n",
       "      <td>58</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pred_7</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pred_8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        true_3  true_4  true_5  true_6  true_7  true_8\n",
       "pred_3       0       0       0       0       0       0\n",
       "pred_4       0       0       1       3       0       0\n",
       "pred_5       1       6      99      62       8       1\n",
       "pred_6       0       3      34      58      26       1\n",
       "pred_7       1       2       2       5       6       1\n",
       "pred_8       0       0       0       0       0       0"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Matriz de Confusão\n",
    "confm = metrics.confusion_matrix(Y_test, Yhat, labels=outcome_labels)\n",
    "confmT = confm.T\n",
    "\n",
    "wine_ConfusionMatrix = pd.DataFrame(confmT)\n",
    "wine_ConfusionMatrix.columns = ['true_' + str(val) for val in outcome_labels]\n",
    "wine_ConfusionMatrix.index   = ['pred_' + str(val) for val in outcome_labels]\n",
    "wine_ConfusionMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Matriz de Confusão Normalizada\n",
    "# sumPerActualQuality = wine_ConfusionMatrix.apply(sum, axis=0)\n",
    "# print(sumPerActualQuality)\n",
    "# dfWineCMNormalizedByActualValue = wine_ConfusionMatrix.apply((lambda x: x/sumPerActualQuality), axis=1)\n",
    "# dfWineCMNormalizedByActualValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP {3: 0, 4: 0, 5: 99, 6: 58, 7: 6, 8: 0}\n",
      "FP {3: 0, 4: 4, 5: 78, 6: 64, 7: 11, 8: 0}\n",
      "FN {3: 2, 4: 11, 5: 37, 6: 70, 7: 34, 8: 3}\n",
      "TN {3: 318, 4: 305, 5: 106, 6: 128, 7: 269, 8: 317}\n"
     ]
    }
   ],
   "source": [
    "# Definindo TP, TN, FP e FN para a matriz de confusão\n",
    "# FIXME: Não sei se para a avaliação usamos a matriz normal gerada ou a normalizada, por enquanto estou usando a normal\n",
    "\n",
    "TP = {}\n",
    "for i in outcome_labels:\n",
    "    predicted = 'pred_' + str(i)\n",
    "    real = 'true_' + str(i)\n",
    "    # Marcar como \"true positive\" os elementos que foram preditos como i e eram de fato i\n",
    "    TP[i] = wine_ConfusionMatrix.loc[predicted, real]\n",
    "    \n",
    "FP = {}\n",
    "for i in outcome_labels:\n",
    "    predicted = 'pred_' + str(i)\n",
    "    real = 'true_' + str(i)\n",
    "    FP[i] = 0\n",
    "    j = 0\n",
    "    # Para os elementos que foram preditos como i\n",
    "    for elements_value in wine_ConfusionMatrix.loc[predicted]:\n",
    "        elements = 'true_' + str(outcome_labels[j])\n",
    "        # Mas não forem de fato i, marcar como \"false positive\"\n",
    "        if(elements != real):\n",
    "            FP[i] += elements_value\n",
    "        j += 1\n",
    "        \n",
    "FN = {}\n",
    "for i in outcome_labels:\n",
    "    predicted = 'pred_' + str(i)\n",
    "    real = 'true_' + str(i)\n",
    "    FN[i] = 0\n",
    "    j = 0\n",
    "    # Para os elementos que eram de fato i\n",
    "    for elements_value in wine_ConfusionMatrix.loc[:,real]:\n",
    "        elements = 'pred_' + str(outcome_labels[j])\n",
    "        # Mas não foram preditos como i, marcar como \"false negative\"\n",
    "        if(elements != predicted):\n",
    "            FN[i] += elements_value\n",
    "        j += 1\n",
    "\n",
    "TN = {}\n",
    "for i in outcome_labels:\n",
    "    predicted = 'pred_' + str(i)\n",
    "    real = 'true_' + str(i)\n",
    "    TN[i] = 0\n",
    "    # Para todos os elementos\n",
    "    for index, row in wine_ConfusionMatrix.iterrows():\n",
    "        # Que não foram preditos como i\n",
    "        if(index != predicted):\n",
    "            for j in outcome_labels:\n",
    "                elements = 'true_' + str(j)\n",
    "                # E que de fato não eram i, marcar como \"true negative\"\n",
    "                if(elements != real):\n",
    "                    elements_value = row[elements]\n",
    "                    TN[i] += elements_value\n",
    "                    \n",
    "print('TP', TP)\n",
    "print('FP', FP)\n",
    "print('FN', FN)\n",
    "print('TN', TN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# PRECISION\n",
      "precision for wines of quality 3        nan\n",
      "precision for wines of quality 4     0.0000\n",
      "precision for wines of quality 5     0.5593\n",
      "precision for wines of quality 6     0.4754\n",
      "precision for wines of quality 7     0.3529\n",
      "precision for wines of quality 8        nan\n",
      "\n",
      "# RECALL / SENSITIVITY\n",
      "recall for wines of quality 3     0.0000\n",
      "recall for wines of quality 4     0.0000\n",
      "recall for wines of quality 5     0.7279\n",
      "recall for wines of quality 6     0.4531\n",
      "recall for wines of quality 7     0.1500\n",
      "recall for wines of quality 8     0.0000\n",
      "\n",
      "# ACCURACY\n",
      "accuracy     0.5191\n",
      "163 1443\n",
      "accuracy      0.5094 (from sklearn.metrics)\n",
      "\n",
      "# F1 SCORE\n",
      "F1 Score for wines of quality 3        nan\n",
      "F1 Score for wines of quality 4        nan\n",
      "F1 Score for wines of quality 5     0.6326\n",
      "F1 Score for wines of quality 6     0.4640\n",
      "F1 Score for wines of quality 7     0.2105\n",
      "F1 Score for wines of quality 8        nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabi_\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:8: RuntimeWarning: invalid value encountered in long_scalars\n",
      "C:\\Users\\gabi_\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:54: RuntimeWarning: invalid value encountered in long_scalars\n",
      "C:\\Users\\gabi_\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:56: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "# Métricas de Qualidade\n",
    "\n",
    "# Precision by label in outcome_labels\n",
    "# precision = TP / (TP + FP)\n",
    "\n",
    "print(\"# PRECISION\")\n",
    "for i in outcome_labels:\n",
    "    precision = (TP[i] / (TP[i] + FP[i]))\n",
    "    print('precision for wines of quality ' + str(i) + '   ', '{:7.4f}'.format(precision))\n",
    "\n",
    "# print('precision   ', '{:7.4f}'.format(metrics.precision_score(Y_test, Yhat)), '(from sklearn.metrics)')\n",
    "\n",
    "# Recall/Sensitivity by label in outcome_labels\n",
    "# recall/sensitivity = TP / (TP + FN)\n",
    "\n",
    "print(\"\\n# RECALL / SENSITIVITY\")\n",
    "for i in outcome_labels:\n",
    "    recall = (TP[i] / (TP[i] + FN[i]))\n",
    "    print('recall for wines of quality ' + str(i) + '   ', '{:7.4f}'.format(recall))\n",
    "\n",
    "# print('recall      ', '{:7.4f}'.format(metrics.recall_score(Y_test, Yhat)), '(from sklearn.metrics)')\n",
    "\n",
    "# Accuracy\n",
    "# accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "print(\"\\n# ACCURACY\")\n",
    "\n",
    "def sum_dict_values (dict_values):\n",
    "    sum_ = 0\n",
    "    for i in outcome_labels:\n",
    "        sum_ += dict_values[i]\n",
    "    if(math.isnan(sum_)):\n",
    "        return 0\n",
    "    else:\n",
    "        return sum_\n",
    "    \n",
    "sumTP = sum_dict_values(TP)\n",
    "sumTN = sum_dict_values(TN)\n",
    "sumFP = sum_dict_values(FP)\n",
    "sumFN = sum_dict_values(FN)\n",
    "\n",
    "# accuracy = ((sumTP + sumTN) / (sumTP + sumTN + sumFP + sumFN))\n",
    "accuracy = ((sumTP ) / (sumFP + sumFN))\n",
    "print('accuracy' + '   ', '{:7.4f}'.format(accuracy))\n",
    "print(sumTP, sumTN)\n",
    "\n",
    "print('accuracy    ', '{:7.4f}'.format(metrics.accuracy_score(Y_test, Yhat)), '(from sklearn.metrics)')\n",
    "\n",
    "# F1 Score by label in outcome_labels\n",
    "# F1_score = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "print(\"\\n# F1 SCORE\")\n",
    "for i in outcome_labels:\n",
    "    precision = (TP[i] / (TP[i] + FP[i]))\n",
    "    recall = (TP[i] / (TP[i] + FN[i]))\n",
    "    F1_score = ((2 * precision * recall) / (precision + recall))\n",
    "    print('F1 Score for wines of quality ' + str(i) + '   ', '{:7.4f}'.format(F1_score))\n",
    "\n",
    "# print('F1_score    ', '{:7.4f}'.format(metrics.f1_score(Y_test, Yhat)), '(from sklearn.metrics)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
